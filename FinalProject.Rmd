---
title: "FINAL"
author: "Andrew Dela Cruz"
date: "12/5/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(randomForest)
library(tree)
library(caret)
library(GGally)
library(rpart)
library(stringr)
library(ROCR)
library(rpart.plot)
library(ipred)
library(randomForest)
```

###Loading Data
```{r}
raw_data = read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",
                sep = ',', stringsAsFactors = F)

test_set = read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test",
                      sep = ',', skip = 1, stringsAsFactors = F)

test_set$V15 = str_replace(test_set$V15, "\\.", "")

AllData = rbind(raw_data, test_set)

column_names = c("Age", "WorkClass", "FNLWGT", "Education", "EducationNum", "MaritalStatus", "Occupation",
          "Relationship", "Race", "Sex", "CapitalGain", "CapitalLoss", "HoursWeek", 
          "NativeCountry", "Salary")
colnames(AllData) = column_names

ColClasses = sapply(AllData, class)

```

#Exploratory Data Analysis

##Missing Values
```{r}
#Removing empty space at start, replacing ? with unknown
char_vars = names(AllData)[ColClasses=='character']
for (var in char_vars){
  AllData[,var] = str_trim(AllData[,var])
  AllData[AllData[,var]=='?', var] = 'Unknown'
  AllData[,var] = factor(AllData[,var])
}

nas = AllData %>%
  filter(Occupation =="Unknown")
summary(nas)

AllData = AllData %>%
  filter(NativeCountry != 'Unknown')
```

Missing values:
  The majority of missing values come from the Occupation and WorkingClass columns. There is almost a complete overlap between those without information on occupation and those without info on working class. The only other column with missing values is `NativeCountry`. 
  
By comparing summary statistics for those with missing values, we can see that the distribution of the other statistics does not vary greatly between those with missing values and those without. (Slight changes in Education, Marital Status) However, we notice that 90% of people with missing values make less than 50k `Salary` as compared to the base rate of 76% in the general population. 

##Outliers
See binning. 

##Binning
```{r}
# #We investigated binning this to see if it would improve results, it did not.
# mean(AllData$CapitalGain>0)
# mean(AllData$CapitalLoss>0)
# CapitalChange = AllData$CapitalGain-AllData$CapitalLoss
# 
# AllData$Capital = NA
# AllData[CapitalChange==0,'Capital'] = 'None'
# AllData[CapitalChange<0,'Capital'] = 'Loss'
# AllData[CapitalChange>0,'Capital'] = 'Gain'
# AllData[CapitalChange==99999,'Capital'] = 'Max'
# AllData[,c('CapitalGain', 'CapitalLoss')] = NULL
# AllData$Capital = factor(AllData$Capital)
```

The columns `CapitalGain` and `CapitalLoss` can be combined, since a value in Gain automatically means there is a value in Loss, so we combine it into `CapitalChange`.

We can see that only 8% of people have `CapitalChange` over 100, 5% have less than -100, and 87% have exactly 0. Also, a few hundred cases have a `CapitalChange` value of 99999 which appears to be a max cap rather than a true value. Thus, we choose to bin `CapitalChange` into 4 levels: loss, none, gain, and max

Furthermore, notice that `NativeCountry` has over 42 levels, meaning it cannot be used in tree building unless we reduce that number, so we will group by continent.

```{r}
asia = c('Cambodia', 'China','Hong', 'India', 'Iran', 'Japan', 'Laos', 'Taiwan', 'Thailand', 'Vietnam', 'South')
north_america = c('Canada', 'United-States')
south_america = c('Columbia', 'Cuba', 'Dominican-Republic', 'Ecuador' , 'El-Salvador', 'Guatemala', 'Haiti', 'Honduras', 'Jamaica', 'Mexico', 'Nicaragua', 'Outlying-US(Guam-USVI-etc)', 'Peru', 'Philippines', 'Puerto-Rico','Trinidad&Tobago')
europe = c('England', 'France', 'Germany', 'Greece','Holand-Netherlands', 'Hungary', 'Ireland', 'Italy', 'Poland', 'Portugal', 'Scotland', 'Yugoslavia')

countries = as.character(AllData$NativeCountry)
AllData$NativeCountry = countries
AllData[countries %in% asia, 'NativeCountry'] = 'Asia'
AllData[countries %in% north_america, 'NativeCountry'] = 'North America'
AllData[countries %in% south_america, 'NativeCountry'] = 'South America'
AllData[countries %in% europe, 'NativeCountry'] = 'Europe'
AllData$NativeCountry = factor(AllData$NativeCountry)
```


##Changing Scales
```{r}
numerics = (sapply(AllData, class) == "integer")
AllData[,numerics] = scale(AllData[,numerics])
sapply(AllData[,numerics], sd)
```
Numeric columns of data are now mean-centered and scaled

##Dummy Indicators
Classification trees do not require one-hot encoding

##Summary Statistics
```{r}
summary(AllData)
```

##Visualizing Distributions
```{r}
num_vars = names(AllData)[numerics]

for(var in num_vars){
  hist(AllData[,var], main = var, xlab = var)
}

non_num_vars = names(AllData)[!numerics][1:7]

for (var in non_num_vars){
  plot(AllData[,var])
}
```

##Adjustments
```{r}
```


##Splitting Data
```{r}
set.seed(2)
AllData$EducationNum = NULL

AllData$Salary = factor(ifelse(as.character(AllData$Salary) == '<=50K', 'No', 'Yes'))

temp = factor(AllData$Salary, levels(AllData$Salary)[c(2,1)])
AllData$Salary = temp

Data = AllData[1:nrow(raw_data),]
Validation = AllData[(nrow(raw_data)+1):nrow(AllData),]

trainIndex = sample(nrow(Data),0.8*nrow(Data) )
Train = Data[trainIndex,]
test = Data[-trainIndex,]
```

#Classification Tree

##Using tree()
```{r}
set.seed(3)
tree.Salary = tree(Salary~., Train)
Salary.predict = predict(tree.Salary, Train, type = 'class')
confusionMatrix(Salary.predict, Train$Salary)

plot(tree.Salary, main = 'Original Tree from tree()')
text(tree.Salary)
```

##Tuning tree()
```{r}
cv.Salary = cv.tree(tree.Salary, FUN = prune.misclass)

par(mfrow = c(1,2))
plot(cv.Salary$size, cv.Salary$dev, type = 'b')
plot(cv.Salary$k, cv.Salary$dev, type = 'b')
```

Deviation is min at 5 and 8

Deviation is min at small k (k=0)

##Pruning tree
```{r}
prune.8 = prune.misclass(tree.Salary, best = 8)
prune.5 = prune.misclass(tree.Salary, best = 5)

tree.pred8 = predict(prune.8,test, type = 'class')
tree.pred5 = predict(prune.5,test, type = 'class')
confusion.8 = confusionMatrix(tree.pred8,test$Salary)
confusion.5 = confusionMatrix(tree.pred5,test$Salary)
confusion.8
confusion.5
summary(prune.5)
plot(prune.5)
text(prune.5)
```


Now the same thing in rpart

##rpart()

##Tuning complexity then depth
```{r}
#tuned to optimal complexity parameter
rpartTune = train(Train[,-ncol(Train)], Train[,"Salary"],
                  method = c("rpart"),
                  tuneLength = 10,
                  trControl = trainControl(method = "cv"))

best.cp = rpartTune$results[which.max(rpartTune$results$Accuracy), 'cp']

#Then depth
rpartTuneDepth = train(Train[,-ncol(Train)], Train[,"Salary"],
                  method = c("rpart2"),
                  tuneLength = 10,
                  trControl = trainControl(method = "cv"), control = rpart.control(cp = best.cp))

best.depth = rpartTuneDepth$results[which.max(rpartTuneDepth$results$Accuracy), 'maxdepth']

tuning.params = rpart.control(cp = best.cp, maxdepth = best.depth)

final.tree = rpart(Salary ~ ., data = Train, control = tuning.params)

posteriors.Tree = predict(final.tree, test, type = 'prob')
predic.Tree = predict(final.tree, newdata = test, type = 'class')
Conf.Tree = confusionMatrix(test$Salary, predic.Tree)
Conf.Tree

prp(final.tree)
```

##Tuning depth then complexity
```{r}
rpartTune2 = train(Train[,-ncol(Data)], Train[,"Salary"],
                  method = "rpart2",
                  tuneLength = 10,
                  trControl = trainControl(method = "cv"))
rpartTune2

best.depth2 = rpartTune2$results[which.max(rpartTune2$results$Accuracy), 'maxdepth']

rpartTune2cp = train(Train[,-ncol(Train)], Train[,"Salary"],
                  method = c("rpart"),
                  tuneLength = 10,
                  trControl = trainControl(method = "cv"), control = rpart.control(maxdepth = best.depth))

best.cp2 = rpartTune$results[which.max(rpartTune$results$Accuracy), 'cp']

tuning.params2 = rpart.control(cp = best.cp2, maxdepth = best.depth2)

final.tree2 = rpart(Salary ~ ., data = Train, control = tuning.params2)

posteriors.Tree = predict(final.tree2, test, type = 'prob')
predic.Tree = predict(final.tree2, newdata = test, type = 'class')
Conf.Tree = confusionMatrix(test$Salary, predic.Tree)
Conf.Tree
```

ROC/AUC
```{r}
final.tree = rpart(Salary ~ ., data = Train, control = tuning.params)

posteriors.Tree = predict(final.tree, test, type = 'prob')
predic.Tree = predict(final.tree, newdata = test, type = 'class')
Conf.Tree = confusionMatrix(test$Salary, predic.Tree)

df.Tree = cbind(posteriors.Tree[,2], test$Salary)
preds.Tree = prediction(df.Tree[,1], df.Tree[,2])
ROC.Tree = performance(preds.Tree, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Tree)
abline(a = 0, b = 1 , lty = 2)
auc.Tree = performance(preds.Tree, measure = 'auc')
auc.Tree@y.values[[1]]
```


##Variable Importance
```{r}
imp = varImp(final.tree) 
importance = data.frame(var = rownames(imp), importance = imp$Overall)
importance %>%
  arrange(desc(importance))
```


###Bagged Tree
```{r}
set.seed(34)
p = ncol(Train)-1

all.trees = c(10,25,50,100,200,500)
results = data.frame(ntree = all.trees, auc = 0)

for (i in 1:length(all.trees)){
  n = all.trees[i]
  forest = randomForest(Salary~.,
                        data = Train,
                        mtry = p,
                        importance = F,
                        ntree = n)
  
  posteriors.Bagged = predict(forest, newdata = test, type = 'prob')
  predic.Bagged = predict(forest, newdata = test, type = 'class')
  df.Bagged = cbind(posteriors.Bagged[,2], test$Salary)
  preds.Bagged = prediction(df.Bagged[,1], df.Bagged[,2])
  ROC.Bagged = performance(preds.Bagged, measure = 'tpr', x.measure = 'fpr')
  auc.Bagged <-performance(preds.Bagged, measure = 'auc')
  results[i,'auc'] = auc.Bagged@y.values[[1]]
  print(i)
}

plot(results)
```

We can see where the number of trees stops helping the model

```{r}
depths = c(2, 50, 100, seq(from = 400, to = 2000, by = 200))

results = data.frame(max.nodes = depths, auc = 0)
for (i in 1:nrow(results)){
  depth = results[i, 'max.nodes']

  forest = randomForest(Salary ~ .,
                        data = Train,
                        mtry = p,
                        importance = F,
                        ntree = 100,
                        maxnodes = depth)
  
  posteriors.Bagged = predict(forest, newdata = test, type = 'prob')
  predic.Bagged = predict(forest, newdata = test, type = 'class')
  df.Bagged = cbind(posteriors.Bagged[,2], test$Salary)
  preds.Bagged = prediction(df.Bagged[,1], df.Bagged[,2])
  ROC.Bagged = performance(preds.Bagged, measure = 'tpr', x.measure = 'fpr')
  auc.Bagged <-performance(preds.Bagged, measure = 'auc')
  results[i,'auc'] = auc.Bagged@y.values[[1]]
  print(i)
}

results = results %>%
  arrange(desc(auc))
head(results)

ggplot(data = results, aes(x = max.nodes, y = auc)) + geom_point() + geom_line(col = 'red')
```

##Best Bagged Tree
```{r}
best.depth = results[1,'max.nodes']

final.bag = randomForest(Salary ~ .,
                         data = Train,
                         mtry = p,
                         importance = T,
                         ntree = 100, 
                         maxnodes = best.depth)

posteriors.Bagged = predict(final.bag, newdata = test, type = 'prob')
predic.Bagged = predict(final.bag, newdata = test, type = 'class')
Conf.Bagged = confusionMatrix(test$Salary, predic.Bagged)
Conf.Bagged

df.Bagged = cbind(posteriors.Bagged[,2], test$Salary)
preds.Bagged = prediction(df.Bagged[,1], df.Bagged[,2])
ROC.Bagged = performance(preds.Bagged, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Bagged)
abline(a=0, b=1, lty=2)

auc.Bagged <-performance(preds.Bagged, measure = 'auc')
auc.Bagged@y.values[[1]]
```

```{r}
importance(final.bag)
varImpPlot(final.bag)
```

#Random Forest
```{r}
depths = c(2, 50, 100, seq(from = 400, to = 1200, by = 200))
mtry = c(4,6,8,10,12)

results = expand.grid(max.nodes = depths, nvar = mtry)
results$auc = 0

for (i in 1:nrow(results)){
  depth = results[i, 'max.nodes']
  nvars = results[i, 'nvar']

  forest = randomForest(Salary ~ .,
                        data = Train,
                        mtry = nvars,
                        importance = F,
                        ntree = 100,
                        maxnodes = depth)
  
  posteriors.Forest = predict(forest, newdata = test, type = 'prob')
  predic.Forest = predict(forest, newdata = test, type = 'class')
  df.Forest = cbind(posteriors.Forest[,2], test$Salary)
  preds.Forest = prediction(df.Forest[,1], df.Forest[,2])
  ROC.Forest = performance(preds.Forest, measure = 'tpr', x.measure = 'fpr')
  auc.Forest <-performance(preds.Forest, measure = 'auc')
  results[i,'auc'] = auc.Forest@y.values[[1]]
  print(i)
}

results = results %>%
  arrange(desc(auc))
head(results)
```


```{r}
best.mtry = results[1,'nvar']
best.depth = results[1, 'max.nodes'] 
final.forest = randomForest(Salary ~ .,
                        data = Train,
                        mtry = best.mtry,
                        importance = T,
                        ntree = 100,
                        maxnodes = best.depth)

posteriors.Forest = predict(final.forest, newdata = test, type = 'prob')
predic.Forest = predict(final.forest, newdata = test, type = 'class')
Conf.Forest = confusionMatrix(test$Salary, predic.Forest)
Conf.Forest
```


```{r}
df.Forest = cbind(posteriors.Forest[,2], test$Salary)
preds.Forest = prediction(df.Forest[,1], df.Forest[,2])
ROC.Forest = performance(preds.Forest, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Forest)
abline(a = 0, b = 1, lty = 2)

auc.Forest <-performance(preds.Forest, measure = 'auc')
auc.Forest@y.values[[1]]
```

```{r}
importance(final.forest)
varImpPlot(final.forest)
```


#Validation

##Classification Tree
```{r}
posteriors.Validate = predict(final.tree, Validation, type = 'prob')
predic.Validate = predict(final.tree, newdata = Validation, type = 'class')
Conf.Validate = confusionMatrix(Validation$Salary, predic.Validate)
Conf.Validate
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Validate)
abline(a = 0, b = 1 , lty = 2)
auc.Validate = performance(preds.Validate, measure = 'auc')
auc.Validate@y.values[[1]]
```

##Bagged Forest
```{r}
posteriors.Validate = predict(final.bag, Validation, type = 'prob')
predic.Validate = predict(final.bag, newdata = Validation, type = 'class')
Conf.Validate = confusionMatrix(Validation$Salary, predic.Validate)
Conf.Validate
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Validate)
abline(a = 0, b = 1 , lty = 2)
auc.Validate = performance(preds.Validate, measure = 'auc')
auc.Validate@y.values[[1]]
```

##Random Forest
```{r}
posteriors.Validate = predict(final.forest, Validation, type = 'prob')
predic.Validate = predict(final.forest, newdata = Validation, type = 'class')
Conf.Validate = confusionMatrix(Validation$Salary, predic.Validate)
Conf.Validate

rf.conf.matrix = Conf.Validate
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Validate)
abline(a = 0, b = 1 , lty = 2)

auc.Validate = performance(preds.Validate, measure = 'auc')
auc.Validate@y.values[[1]]
```

##Sensitivity and Specificity

```{r}
tpr = (conf[2,2]/(conf[2,2]+conf[2,1]))
tnr = (conf[1,1]/(conf[1,1]+conf[1,2]))
```

