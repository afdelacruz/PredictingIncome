---
title: "FINAL"
author: "Andrew Dela Cruz"
date: "12/5/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(randomForest)
library(tree)
library(caret)
library(GGally)
library(rpart)
library(stringr)
library(ROCR)
library(rpart.plot)
```

DATA PREPROCESSING TODO LIST:
-Load data in 
-Create column names 
-Convert everything to correct data types (factors, numeric)
-Check for complete samples
-Check for outliers
-Create dummy variables
-Correlations
-Plots
-Summary Stats



###Loading Data
```{r}
raw_data = read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",
                sep = ',', stringsAsFactors = F)

test_set = read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test",
                      sep = ',', skip = 1, stringsAsFactors = F)

test_set$V15 = str_replace(test_set$V15, "\\.", "")

AllData = rbind(raw_data, test_set)

column_names = c("age", "WorkClass", "FNLWGT", "Education", "EducationNum", "MartialStatus", "Occupation",
          "Relationship", "Race", "Sex", "CapitalGain", "CapitalLoss", "HoursWeek", 
          "NativeCountry", "Salary")
colnames(AllData) = column_names

ColClasses = sapply(AllData, class)

char_vars = names(AllData)[ColClasses=='character']

#Removing empty space at start, replacing ? with na
for (var in char_vars){
  AllData[,var] = str_trim(AllData[,var])
  AllData[AllData[,var]=='?', var] = 'Unknown'
  AllData[,var] = factor(AllData[,var])
}
```

#Exploratory Data Analysis

##Missing Values
```{r}
summary(AllData)

nas = AllData %>%
  filter(Occupation == 'Unknown')
summary(nas)
```

Missing values:
  The majority of missing values come from the Occupation and WorkingClass columns. There is almost a complete overlap between those without information on occupation and those without info on working class. The only other column with missing values is `NativeCountry`. 
  
By comparing summary statistics for those with missing values, we can see that the distribution of the other statistics does not vary greatly between those with missing values and those without. (Slight changes in Education, Marital Status) However, we notice that 90% of people with missing values make less than 50k `Salary` as compared to the base rate of 76% in the general population.
 
```{r}
AllData[is.na(AllData$Occupation), 'Occupation'] = 'Unknown'
```

##Outliers

##Binning
```{r}
#We investigated binning this to see if it would improve results, it did not.
# mean(AllData$CapitalGain>0)
# mean(AllData$CapitalLoss>0)
# CapitalChange = AllData$CapitalGain-AllData$CapitalLoss
# 
# AllData$Capital = NA
# AllData[CapitalChange==0,'Capital'] = 'None'
# AllData[CapitalChange<0,'Capital'] = 'Loss'
# AllData[CapitalChange>0,'Capital'] = 'Gain'
# AllData[CapitalChange==99999,'Capital'] = 'Max'
# AllData[,c('CapitalGain', 'CapitalLoss')] = NULL
# AllData$Capital = factor(AllData$Capital)
```

The columns `CapitalGain` and `CapitalLoss` can be combined, since a value in Gain automatically means there is a value in Loss, so we combine it into `CapitalChange`.

We can see that only 8% of people have `CapitalChange` over 100, 5% have less than -100, and 87% have exactly 0. Also, a few hundred cases have a `CapitalChange` value of 99999 which appears to be a max cap rather than a true value. Thus, we choose to bin `CapitalChange` into 4 levels: loss, none, gain, and max

Furthermore, notice that `NativeCountry` has over 42 levels, meaning it cannot be used in tree building unless we reduce that number, so we will group by continent.

```{r}
asia = c('Cambodia', 'China','Hong', 'India', 'Iran', 'Japan', 'Laos', 'Taiwan', 'Thailand', 'Vietnam', 'South')
north_america = c('Canada', 'United-States')
south_america = c('Columbia', 'Cuba', 'Dominican-Republic', 'Ecuador' , 'El-Salvador', 'Guatemala', 'Haiti', 'Honduras', 'Jamaica', 'Mexico', 'Nicaragua', 'Outlying-US(Guam-USVI-etc)', 'Peru', 'Philippines', 'Puerto-Rico','Trinidad&Tobago')
europe = c('England', 'France', 'Germany', 'Greece','Holand-Netherlands', 'Hungary', 'Ireland', 'Italy', 'Poland', 'Portugal', 'Scotland', 'Yugoslavia')

countries = as.character(AllData$NativeCountry)
AllData$NativeCountry = countries
AllData[countries %in% asia, 'NativeCountry'] = 'Asia'
AllData[countries %in% north_america, 'NativeCountry'] = 'North America'
AllData[countries %in% south_america, 'NativeCountry'] = 'South America'
AllData[countries %in% europe, 'NativeCountry'] = 'Europe'
AllData$NativeCountry = factor(AllData$NativeCountry)
```


##Changing Scales
```{r}
numerics = (sapply(AllData, class) == "integer")
AllData[,numerics] = scale(AllData[,numerics])
sapply(AllData[,numerics], sd)
```
Numeric columns of data are now mean-centered and scaled

##Dummy Indicators

##Summary Statistics
```{r}
summary(AllData)
```

##Visualizing Distributions
```{r}
num_vars = names(AllData)[numerics]

for(var in num_vars){
  hist(AllData[,var], main = var, xlab = var)
}

non_num_vars = names(AllData)[!numerics][1:7]

for (var in non_num_vars){
  plot(AllData[,var])
}
```

##Adjustments
```{r}
```

#Association between predictor and response

##Split Data
```{r}
set.seed(2)
AllData$EducationNum = NULL

Data = AllData[1:nrow(raw_data),]
Validation = AllData[(nrow(raw_data)+1):nrow(AllData),]

trainIndex = sample(nrow(Data),0.8*nrow(Data) )
Train = Data[trainIndex,]
test = Data[-trainIndex,]
```

#Classification Tree
```{r}
tree.Salary = tree(Salary~., Train)
Salary.predict = predict(tree.Salary, Train, type = 'class')
confusionMatrix(Salary.predict, Train$Salary)

plot(tree.Salary)
text(tree.Salary)
```


```{r}
set.seed(3)

cv.Salary = cv.tree(tree.Salary, FUN = prune.misclass)

par(mfrow = c(1,2))
plot(cv.Salary$size, cv.Salary$dev, type = 'b')
plot(cv.Salary$k, cv.Salary$dev, type = 'b')
```

Deviation is min at 5 and 8

Deviation is min at small k

```{r}
prune.8 = prune.misclass(tree.Salary, best = 8)
prune.5 = prune.misclass(tree.Salary, best = 5)

tree.pred8 = predict(prune.8,test, type = 'class')
tree.pred5 = predict(prune.5,test, type = 'class')
confusion.8 = confusionMatrix(tree.pred8,test$Salary)
confusion.5 = confusionMatrix(tree.pred5,test$Salary)
confusion.8
confusion.5
summary(prune.5)
plot(prune.5)
text(prune.5)
```

we've built a classification tree with tree, and pruned

now we're gonna do the same thing in rpart

###Single Tree

##adjusting complexity parameter
```{r}

#tuned to optimal complexity parameter
rpartTune = train(Train[,-ncol(Train)], Train[,"Salary"],
                  method = c("rpart"),
                  tuneLength = 10,
                  trControl = trainControl(method = "cv"))

rpartTune 

best.cp = rpartTune$results[which.max(rpartTune$results$Accuracy), 'cp']

rpartTuneDepth = train(Train[,-ncol(Train)], Train[,"Salary"],
                  method = c("rpart2"),
                  tuneLength = 10,
                  trControl = trainControl(method = "cv"), control = rpart.control(cp = best.cp))

best.depth = rpartTuneDepth$results[which.max(rpartTuneDepth$results$Accuracy), 'maxdepth']

tuning.params = rpart.control(cp = best.cp, maxdepth = best.depth)

final.tree = rpart(Salary ~ ., data = Train, control = tuning.params)

posteriors.Tree = predict(final.tree, test, type = 'prob')
predic.Tree = predict(final.tree, newdata = test, type = 'class')
Conf.Tree = confusionMatrix(test$Salary, predic.Tree)
Conf.Tree

ok = summary(rpartTune)
prp(final.tree)
```



ROC/AUC
```{r}
df.Tree = cbind(posteriors.Tree[,2], test$Salary)
preds.Tree = prediction(df.Tree[,1], df.Tree[,2])
ROC.Tree = performance(preds.Tree, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Tree)
abline(a = 0, b = 1 , lty = 2)
auc.Tree = performance(preds.Tree, measure = 'auc')
auc.Tree@y.values
```


#Adjusting max depth 
```{r}
#better = rpart(Salary ~., data = Data, cp = 0.001530417)
rpartTune2 = train(Data[,-ncol(Data)], Data[,"Salary"],
                  method = "rpart2",
                  tuneLength = 10,
                  trControl = trainControl(method = "cv"))
rpartTune2
#rpart.Prune = prune(rpartTune, cp = 5, "CP")
posteriors.Tree = predict(rpartTune2, test, type = 'prob', cp = 10)
predic.Tree = predict(rpartTune, newdata = test)
Conf.Tree = confusionMatrix(test$Salary, predic.Tree)
Conf.Tree
df.Tree = cbind(posteriors.Tree[,2], test$Salary)
preds.Tree = prediction(df.Tree[,1], df.Tree[,2])
ROC.Tree = performance(preds.Tree, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Tree)
abline(a = 0, b = 1 , lty = 2)
auc.Tree = performance(preds.Tree, measure = 'auc')
auc.Tree@y.values

```

Tuning complexity paramter increases accuracy more than adjusting depth. 


###Bagged Tree
```{r}
set.seed(34)
library(ipred)
library(randomForest)
p = ncol(Train)-1
#bag.Salary = bagging(Salary ~., data = Train)
#plot(bag.Salary)
bag.Salary = randomForest(Salary~.,
                          data = Train, 
                          mtry = p, 
                          importance = TRUE,
                          ntree = 500)
posteriors.Bagged = predict(bag.Salary, newdata = test, type = 'prob')
predic.Bagged = predict(bag.Salary, newdata = test, type = 'class')
Conf.Bagged = confusionMatrix(test$Salary, predic.Bagged)
Conf.Bagged
df.Bagged = cbind(posteriors.Bagged[,2], test$Salary)
preds.Bagged = prediction(df.Bagged[,1], df.Bagged[,2])
ROC.Bagged = performance(preds.Bagged, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Bagged)
abline(a = 0, b = 1, lty = 2)
auc.Bagged <-performance(preds.Bagged, measure = 'auc')
auc.Bagged@y.values
importance(bag.Salary)
varImpPlot(bag.Salary)
```

```{r}
set.seed(29)
library(ROCR)
rfModel = tuneRF(Train[,-ncol(Train)], Train[,ncol(Train)], 
                 stepFactor=1.5,
                 doBest = TRUE,
                 improve=1e-5, 
                 ntree = 500)

summary(rfModel)
posteriors.rf = predict(rfModel, test, type = 'prob')
predic.rf = predict(rfModel, newdata = test)
Conf.rf = confusionMatrix(test$Salary, predic.rf)
Conf.rf
df.rf = cbind(posteriors.rf[,2], test$Salary)
preds.rf = prediction(df.rf[,1], df.rf[,2])
ROC.rf = performance(preds.rf, measure = 'tpr', x.measure = 'fpr')
plot(ROC.rf)
abline(a = 0, b = 1 , lty = 2)
auc.rf = performance(preds.rf, measure = 'auc')
auc.rf@y.values
importance(rfModel)
varImpPlot(rfModel)

```

###Tune RF with Caret but takes awhile
```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(2)
mtry <- sqrt(ncol(Train[,1:ncol(Train)]))
rf_random <- train(Salary~., data=Train, method="rf", metric="Accuracy", tuneLength=5, trControl=control)
print(rf_random)
plot(rf_random)
```


###Showing all models on Validation Set
```{r}
posteriors.Validate = predict(rpartTune, Validation, type = 'prob')
predic.Validate = predict(rfModel, newdata = Validation)
Conf.Validate = confusionMatrix(Validation$Salary, predic.Validate)
Conf.Validate
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Validate)
abline(a = 0, b = 1 , lty = 2)
auc.Validate = performance(preds.Validate, measure = 'auc')
auc.Validate@y.values
```

```{r}
posteriors.Validate = predict(bag.Salary, Validation, type = 'prob')
predic.Validate = predict(rfModel, newdata = Validation)
Conf.Validate = confusionMatrix(Validation$Salary, predic.Validate)
Conf.Validate
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Validate)
abline(a = 0, b = 1 , lty = 2)
auc.Validate = performance(preds.Validate, measure = 'auc')
auc.Validate@y.values
```

```{r}
posteriors.Validate = predict(rfModel, Validation, type = 'prob')
predic.Validate = predict(rfModel, newdata = Validation)
Conf.Validate = confusionMatrix(Validation$Salary, predic.Validate)
Conf.Validate
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Validate)
abline(a = 0, b = 1 , lty = 2)
auc.Validate = performance(preds.Validate, measure = 'auc')
auc.Validate@y.values
```