---
title: "FINAL"
author: "Andrew Dela Cruz"
date: "12/5/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dummies)
library(randomForest)
library(tree)
library(caret)
library(GGally)
library(rpart)
library(stringr)
```

DATA PREPROCESSING TODO LIST:
-Load data in 
-Create column names 
-Convert everything to correct data types (factors, numeric)
-Check for complete samples
-Check for outliers
-Create dummy variables
-Correlations
-Plots
-Summary Stats

```{r}
raw_data = read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",
                sep = ',')
column_names = c("age", "WorkClass", "FNLWGT", "Education", "EducationNum", "MartialStatus", "Occupation",
          "Relationship", "Race", "Sex", "CapitalGain", "CapitalLoss", "HoursWeek", 
          "NativeCountry", "Salary")

test_set = read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test",
                      sep = ',', skip = 1)

test_set$V15 = str_replace(test_set$V15, "\\.", "")
test_set$V15
AllData = rbind(raw_data, test_set)
colnames(AllData) = column_names
which(complete.cases(AllData) == FALSE)
summary(AllData)

```
No missing values 

```{r}
ColClasses = sapply(AllData, class)
numerics = ColClasses == "integer"
AllData[,numerics] = scale(AllData[,numerics])
AllData[,numerics]
sapply(AllData[,numerics], sd)
```
Numeric columns of data is now mean-centered and scaled

Since we are using trees, it is unneccessary to create dummy variables. Also, each factor has several levels so we did not want our matrix to become too large. 


```{r}
summary(AllData)
```
Age, WorkingClass, Education-Years, Race, Sex, Capital Gain, Salary
```{r}
useful = c('age', 'WorkClass', 'EducationNum', 'Race', 'Sex', 'CapitalGain', 'Salary')
#ggpairs(master.df[,useful])

```

#Classification Tree
```{r}
set.seed(2)
AllData$NativeCountry = NULL
AllData$EducationNum = NULL
Data = AllData[1:nrow(raw_data),]
Validation = AllData[(nrow(raw_data)+1):nrow(AllData),]


trainIndex = sample(nrow(Data),0.8*nrow(Data) )
Train = Data[trainIndex,]
test = Data[-trainIndex,]

tree.Salary = tree(Salary~., Train)
Salary.predict = predict(tree.Salary, Train, type = 'class')
confusionMatrix(Salary.predict, Train$Salary)
```

```{r}
set.seed(3)
class(tree.Salary)
cv.Salary = cv.tree(tree.Salary, FUN = prune.misclass)
names(cv.Salary)
par(mfrow = c(1,2))
plot(cv.Salary$size, cv.Salary$dev, type = 'b')
plot(cv.Salary$k, cv.Salary$dev, type = 'b')
```
```{r}
prune.8 = prune.misclass(tree.Salary, best = 8)
prune.5 = prune.misclass(tree.Salary, best = 5)

tree.pred8 = predict(prune.8,test, type = 'class')
tree.pred5 = predict(prune.5,test, type = 'class')
confusion.8 = confusionMatrix(tree.pred8,test$Salary)
confusion.5 = confusionMatrix(tree.pred5,test$Salary)
confusion.5
summary(prune.5)
```


###Single Tree
##adjusting complexity parameter
```{r}
#better = rpart(Salary ~., data = Data, cp = 0.001530417)
rpartTune = train(Data[,-ncol(Data)], Data[,"Salary"],
                  method = "rpart",
                  tuneLength = 10,
                  trControl = trainControl(method = "cv"))
rpartTune 
#rpart.Prune = prune(rpartTune, cp = 5, "CP")
posteriors.Tree = predict(rpartTune, test, type = 'prob', cp = 10)
predic.Tree = predict(rpartTune, newdata = test)
Conf.Tree = confusionMatrix(test$Salary, predic.Tree)
Conf.Tree
df.Tree = cbind(posteriors.Tree[,2], test$Salary)
preds.Tree = prediction(df.Tree[,1], df.Tree[,2])
ROC.Tree = performance(preds.Tree, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Tree)
abline(a = 0, b = 1 , lty = 2)
auc.Tree = performance(preds.Tree, measure = 'auc')
auc.Tree@y.values

```
#Adjusting max depth 
```{r}
#better = rpart(Salary ~., data = Data, cp = 0.001530417)
rpartTune2 = train(Data[,-ncol(Data)], Data[,"Salary"],
                  method = "rpart2",
                  tuneLength = 10,
                  trControl = trainControl(method = "cv"))
rpartTune2
#rpart.Prune = prune(rpartTune, cp = 5, "CP")
posteriors.Tree = predict(rpartTune2, test, type = 'prob', cp = 10)
predic.Tree = predict(rpartTune, newdata = test)
Conf.Tree = confusionMatrix(test$Salary, predic.Tree)
Conf.Tree
df.Tree = cbind(posteriors.Tree[,2], test$Salary)
preds.Tree = prediction(df.Tree[,1], df.Tree[,2])
ROC.Tree = performance(preds.Tree, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Tree)
abline(a = 0, b = 1 , lty = 2)
auc.Tree = performance(preds.Tree, measure = 'auc')
auc.Tree@y.values

```

Tuning complexity paramter increases accuracy more than adjusting depth. 


###Bagged Tree
```{r}
set.seed(34)
library(ipred)
library(randomForest)
p = ncol(Train)-1
#bag.Salary = bagging(Salary ~., data = Train)
#plot(bag.Salary)
bag.Salary = randomForest(Salary~.,
                          data = Train, 
                          mtry = p, 
                          importance = TRUE,
                          ntree = 500)
posteriors.Bagged = predict(bag.Salary, newdata = test, type = 'prob')
predic.Bagged = predict(bag.Salary, newdata = test, type = 'class')
Conf.Bagged = confusionMatrix(test$Salary, predic.Bagged)
Conf.Bagged
df.Bagged = cbind(posteriors.Bagged[,2], test$Salary)
preds.Bagged = prediction(df.Bagged[,1], df.Bagged[,2])
ROC.Bagged = performance(preds.Bagged, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Bagged)
abline(a = 0, b = 1, lty = 2)
auc.Bagged <-performance(preds.Bagged, measure = 'auc')
auc.Bagged@y.values
importance(bag.Salary)
varImpPlot(bag.Salary)
```

```{r}
set.seed(29)
library(ROCR)
rfModel = tuneRF(Train[,-ncol(Train)], Train[,ncol(Train)], 
                 stepFactor=1.5,
                 doBest = TRUE,
                 improve=1e-5, 
                 ntree = 500)

summary(rfModel)
posteriors.rf = predict(rfModel, test, type = 'prob')
predic.rf = predict(rfModel, newdata = test)
Conf.rf = confusionMatrix(test$Salary, predic.rf)
Conf.rf
df.rf = cbind(posteriors.rf[,2], test$Salary)
preds.rf = prediction(df.rf[,1], df.rf[,2])
ROC.rf = performance(preds.rf, measure = 'tpr', x.measure = 'fpr')
plot(ROC.rf)
abline(a = 0, b = 1 , lty = 2)
auc.rf = performance(preds.rf, measure = 'auc')
auc.rf@y.values
importance(rfModel)
varImpPlot(rfModel)

```

###Tune RF with Caret but takes awhile
```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(2)
mtry <- sqrt(ncol(Train[,1:ncol(Train)]))
rf_random <- train(Salary~., data=Train, method="rf", metric="Accuracy", tuneLength=5, trControl=control)
print(rf_random)
plot(rf_random)
```


###Showing all models on Validation Set
```{r}
posteriors.Validate = predict(rpartTune, Validation, type = 'prob')
predic.Validate = predict(rfModel, newdata = Validation)
Conf.Validate = confusionMatrix(Validation$Salary, predic.Validate)
Conf.Validate
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Validate)
abline(a = 0, b = 1 , lty = 2)
auc.Validate = performance(preds.Validate, measure = 'auc')
auc.Validate@y.values
```
```{r}
posteriors.Validate = predict(bag.Salary, Validation, type = 'prob')
predic.Validate = predict(rfModel, newdata = Validation)
Conf.Validate = confusionMatrix(Validation$Salary, predic.Validate)
Conf.Validate
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Validate)
abline(a = 0, b = 1 , lty = 2)
auc.Validate = performance(preds.Validate, measure = 'auc')
auc.Validate@y.values
```

```{r}
posteriors.Validate = predict(rfModel, Validation, type = 'prob')
predic.Validate = predict(rfModel, newdata = Validation)
Conf.Validate = confusionMatrix(Validation$Salary, predic.Validate)
Conf.Validate
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Validate)
abline(a = 0, b = 1 , lty = 2)
auc.Validate = performance(preds.Validate, measure = 'auc')
auc.Validate@y.values
```