---
title: "Predicting Income Report"
author: "Landon Kleinbrodt and Andrew de la Cruz"
date: "12/08/2017"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
library(tidyverse)
library(tree)
library(caret)
library(rpart.plot)
library(ROCR)
library(ggplot2)
library(randomForest)
```

```{r, echo = F}
raw_data = read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",
                sep = ',', stringsAsFactors = F)

test_set = read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test",
                      sep = ',', skip = 1, stringsAsFactors = F)

test_set$V15 = str_replace(test_set$V15, "\\.", "")

AllData = rbind(raw_data, test_set)

column_names = c("Age", "WorkClass", "FNLWGT", "Education", "EducationNum", "MaritalStatus", "Occupation",
          "Relationship", "Race", "Sex", "CapitalGain", "CapitalLoss", "HoursWeek", 
          "NativeCountry", "Salary")
colnames(AllData) = column_names

ColClasses = sapply(AllData, class)
```

In this report we will attempt to answer the question: can a classification model built on census data predict whether or not an individul's yearly income exceeds $50,000? Specifically, we will investigated the following classification methods: single tree, bagged forest, and random forest. The performance of these three models will be analyzed and compared to determine the best possible model for the data.

The code behind this report can be seen by examining the .Rmd version of this file, or by inspecting the Appendix.Rmd file included in this folder.

##Data

The data set used in this project is the [Census Income Data Set](https://archive.ics.uci.edu/ml/datasets/Census+Income) from the UCI Machine Learning Repository. This data set was donated by Ronny Kohavi and Barry Becker in 2001 and has been cited in over a dozen papers. The set contains ~49k observations of 14 attributes detailing different pieces of census information. Some are continuous, such as capital-gains and hours-per-week worked, while most are categorical, like education level, race, and relationship status. This data has already been split into a training (2/3) and validation set (1/3). A small subsection of the data is shown below.

```{r, echo = F}
show = c('Age', 'Education', 'Occupation', 'Relationship', 'Race', 'Sex', 'Salary')
head(AllData[, show])
```

##Overview

We will begin with a brief discussion of the data loading and pre processing, followed by an exploratory data analysis. Once finished, we move on to fitting and tuning the three types of models. Finally, we will compare the results of these models - specifically using confusion matrices, accuracy rates, and ROC curves. Finally we will choose our best model and validate it on the included test data, as well as compare its predictive test power to the other models.

#Exploratory Data Analysis

###Loading and Preprocessing

Loading in the data from the UCI repository was straightforward, but once loaded it required some pre-procesing. For example, all the categorical variable values began with an empty space, and the `Salary` column of the test data contained an extra `.` in all its entries. Also, the data contained ~6500 total missing values, denoted by the character `?`. The data are supplied as two files (train and test), which we concatanated so that all preprocessing steps were applied equally, and splitting them up again before model training.

###Missing Values

The majority of the missing values come from the `Occupation` and `WorkClass` columns. In fact, there is almost a complete overlap betwen those without information on occupation and those without information on working class, which makes sense as the two categories are quite related. For the most part, the distribution of the other statistics did not vary greatly between those with missing values and those without. Since there were relatively few individuals with missing values, we were first tempted to simply drop those observations with any missing data. However, analysis shows that 90% of people with missing values make less than $50k as ompared to the base rate of 75% in the general population. It would be misrepresentative to rely on imputation to fill in these missing values, especially when we see such a sharp difference in salary distribution. Therefore, we chose to encode missing `Occupation` values as 'Unknown', this allows us to keep the information gained from an unknown occupation (which we see relates to salary) without misrepresenting the individual's status.

The only other variable with missing values is `NativeCountry`, with less than a thousand unknowns. Inspecting this subset reveals that they follow the same distribution as the general population, so it does not appear that excluding this data will bias our models against any specific subset. Thus, since these individuals seem to represent a very small, random subset of the general population we choose to exclude them from the model building process.

```{r, echo = F}
char_vars = names(AllData)[ColClasses=='character']
for (var in char_vars){
  AllData[,var] = str_trim(AllData[,var])
  AllData[AllData[,var]=='?', var] = 'Unknown'
  AllData[,var] = factor(AllData[,var])
}

AllData = AllData %>%
  filter(NativeCountry != 'Unknown')
```

###Outliers and Binning

There were two considerations to make in regards to binning. First, the `CapitalGain` and `CapitalLoss` columns are extremely skewed. 

```{r, echo = F, fig.width = 4, fig.height = 5}
plot(density(AllData$CapitalGain), main = 'CapitalGain Distribution', xlab = 'CapitalGain')
plot(density(AllData$CapitalLoss), main = 'CapitalLoss Distribution', xlab = 'CapitalLoss')
```

First, we noticed that `CapitalGain` and `CapitalLoss` are two sides of the same coin (if CapitalGain>0 then CapitalLoss = 0 and vice versa), and can be combined. This creates a variable `CapitalChange` which is again very skewed: 5% have less than 0, 87% of people have exactly 0, 8% experienced some gain, and .5% experienced the max value. This skewed data seemed ideal for binning, as it was clear those individuals with max `CapitalGain` behaved very differently than those with 0. However, the entirety of this report was run by binning into a single column (into those above specified categories) and by leaving the two columns as they are, and better performance was achieved without binning.

```{r, echo = F}
# #We investigated binning this to see if it would improve results, it did not.
# mean(AllData$CapitalGain>0)
# mean(AllData$CapitalLoss>0)
# CapitalChange = AllData$CapitalGain-AllData$CapitalLoss
# 
# AllData$Capital = NA
# AllData[CapitalChange==0,'Capital'] = 'None'
# AllData[CapitalChange<0,'Capital'] = 'Loss'
# AllData[CapitalChange>0,'Capital'] = 'Gain'
# AllData[CapitalChange==99999,'Capital'] = 'Max'
# AllData[,c('CapitalGain', 'CapitalLoss')] = NULL
# AllData$Capital = factor(AllData$Capital)
```

Furthermore, we can see that `NativeCountry` has over 40 levels, which exceeds the maximum number of many classification models. Thus, we binned this column into continents/regions: Asia, North America, South America/Caribbean, and Europe

```{r, echo = F}
asia = c('Cambodia', 'China','Hong', 'India', 'Iran', 'Japan', 'Laos', 'Taiwan', 'Thailand', 'Vietnam', 'South')
north_america = c('Canada', 'United-States')
south_america = c('Columbia', 'Cuba', 'Dominican-Republic', 'Ecuador' , 'El-Salvador', 'Guatemala', 'Haiti', 'Honduras', 'Jamaica', 'Mexico', 'Nicaragua', 'Outlying-US(Guam-USVI-etc)', 'Peru', 'Philippines', 'Puerto-Rico','Trinidad&Tobago')
europe = c('England', 'France', 'Germany', 'Greece','Holand-Netherlands', 'Hungary', 'Ireland', 'Italy', 'Poland', 'Portugal', 'Scotland', 'Yugoslavia')

countries = as.character(AllData$NativeCountry)
AllData$NativeCountry = countries
AllData[countries %in% asia, 'NativeCountry'] = 'Asia'
AllData[countries %in% north_america, 'NativeCountry'] = 'North America'
AllData[countries %in% south_america, 'NativeCountry'] = 'South America'
AllData[countries %in% europe, 'NativeCountry'] = 'Europe'
AllData$NativeCountry = factor(AllData$NativeCountry)
```


###Changing Scales

While most of the data is categorical, the various numerical variables are on widely different scales. `Age` varies between 17 and 99, `EducationNum` varies between 1 and 16, `HoursWeek` between 1 and 99, and `Capital` gains and losses can be anywhere from 0 to 99,999. This report focuses on tree-based classifers, which are scale invariant, but we chose to center and scale anyways so that other methods of regression/classification could be easily integrated if need be.

```{r, echo = F}
prescale = AllData
numerics = (sapply(AllData, class) == "integer")
AllData[,numerics] = scale(AllData[,numerics])
```

###Dummy Indicators

Tree based classification methods can easily handle categorical variables, and so one-hot-encoding is unecessary. Furthermore, since there are so many categorical variables with so many levels, dummifying them one would greatly increase the number of predictor columns in our data matrix.

###Summary Statistics and Distributions

The summary statistics before and after transformation, as well as the various correlations between predictors were explored and analyzed (figures and code can be found in the appendix). A few points are of note. First, 
many of the categorical are skewed around one mode value. Often this phenomenon likely reflects the trends of the general population: the vast majority of people are Private `WorkClass` and most have either highschool or some college education. Other skews seem to be more results of sampling bias, for example over 85% of participants are white, and over two thirds are male. While it is important to keep these skews in mind, they did not appear to overly interfere with the predictive power of our models.

```{r, echo = F, fig.width = 3, fig.height = 3.5}
num_vars = names(AllData)[numerics]

num_to_show = c('HoursWeek', 'EducationNum')

for(var in num_to_show){
  hist(prescale[,var], main = var, xlab = var)
}

cat_to_show = c('WorkClass', 'Race', 'Sex')

for (var in cat_to_show){
  plot(prescale[,var], main = var)
}
```



Note that there are two variables representing an individuals education: the categorical `Education` and the numerical `EducationNum`. These variables are interchangeable, thus we ran this entire analysis using one and then repeated the process using the other. Superior results were obtained by using `EducationNum` while excluding `Education`

#2) Model Building

After pre-processing and initial exploration, the data was split back into the training and validation sets as provided initially (a 2/3-1/3 split). Furthermore, since we have enough observations, we split the training data again into train (80%) and test (20%) groups. This way, we can use the first test group as a means of comparing the predictive power of our models out of bag, and then we can use the holdout validation set to get a truly indicative performance measure of our final model.

We also specify that those with >$50k are classified as "Yes" and those with less are classified as "No".

```{r, echo = F}
set.seed(2)
AllData$Education = NULL

AllData$Salary = factor(ifelse(as.character(AllData$Salary) == '<=50K', 'No', 'Yes'))

temp = factor(AllData$Salary, levels(AllData$Salary)[c(2,1)])
AllData$Salary = temp

Data = AllData[1:nrow(raw_data),]
Validation = AllData[(nrow(raw_data)+1):nrow(AllData),]

trainIndex = sample(nrow(Data),0.8*nrow(Data) )
Train = Data[trainIndex,]
test = Data[-trainIndex,]
```


For each of the performed methods, you can include a brief description of what each method does (e.g. basic working principles, key ideas, goal), the functions and packages used to carry out the model building process, the main results (tables, summaries, graphs) and their corresponding descriptions and interpretations.

Fit a classification tree (see examples in ISL chapter 8, and APM chapter 14).
Make plots and describe the steps you took to justify choosing optimal tuning parameters.
Report your 5 (or 6 or 7) important features (could be either just 5, or 6 or 7), with their variable importance statistics.
Report the training accuracy rate.
Plot the ROC curve, and report its area under the curve (AUC) statistic.

##Classification Tree

The classification tree is one of the most intuitive and easily readable forms of classification. A tree is a combination of nodes, where each node has a splitting criterion that results in dividing its input group into two output groups based off their values in some variable. At the bottom of the tree are the terminal nodes, the proportions of classes in those terminal nodes decides what the overall classification is for a new individual that ends up in that terminal node. While there are many methods to determine which split is best, the overall goal is to reduce the entropy with each split - to have each new group be more homogenous than before the split.

There are many variations and approaches to building a classification tree, this report explored two of them. First, we used the `tree()` function from the package `tree` to build a base tree (pictured below) 

```{r, echo = F}
set.seed(3)
tree.Salary = tree(Salary~., Train)
Salary.predict = predict(tree.Salary, Train, type = 'class')

plot(tree.Salary, main = 'Original Tree from tree()')
text(tree.Salary)
```

Notice how some of the splits in this tree are redundant, the final split on the far left and far right both result in the same classification. This encourages us to use cross validation to find more optimal parameters. Specifically, we used `cv.tree()` to determine the optimal parameters for tree size and for tree complexity.

```{r, echo = F}
cv.Salary = cv.tree(tree.Salary, FUN = prune.misclass)

par(mfrow = c(1,2))
plot(cv.Salary$size, cv.Salary$dev, type = 'b', ylab = 'Error', xlab = 'Size')
plot(cv.Salary$k, cv.Salary$dev, type = 'b', ylab = 'Error', xlab = 'Complexity Constant')
```

From these plots we can see that error stops decreasing with a tree size of 5, and that error is minimized for small complexity. To be thorough, we pruned our original tree using `prune.misclass`, once with a desired size of 5 and once with a size of 8. Comparing these two reveals that the tree of size 5 performs equally well and is considerably simpler. Thus we see the final result from our `tree()` construction below:

```{r, echo = F}
prune.5 = prune.misclass(tree.Salary, best = 5)

tree.pred5 = predict(prune.5,test, type = 'class')
tree.conf = confusionMatrix(tree.pred5,test$Salary)

plot(prune.5)
text(prune.5)
```

We can see that this tree splits on 3 variables. First it splits based on relationship status, and then by education or capital gain, and then again by capital gain if necessary. Immediately we can see that a low `CapitalGain` is associated with low income as we would expect.

`tree()` is a relatively simple function, and we can obtain more complex results and interpretation using the `rpart` package, which we will do now.

the `rpart` package allows us to train our model using the `train()` function from the `caret` package, by setting `method = 'rpart'`. Using cross validation we tuned the complexity parameter and depth of the tree to result in a depth of 23 and a complexity of ~.002. We then used `rpart` with the specified tuning parameters to arrive at the final tree:

```{r, echo = F}
#tuned to optimal complexity parameter
rpartTune = train(Train[,-ncol(Train)], Train[,"Salary"],
                  method = c("rpart"),
                  tuneLength = 10,
                  trControl = trainControl(method = "cv"))

best.cp = rpartTune$results[which.max(rpartTune$results$Accuracy), 'cp']

#Then depth
rpartTuneDepth = train(Train[,-ncol(Train)], Train[,"Salary"],
                  method = c("rpart2"),
                  tuneLength = 10,
                  trControl = trainControl(method = "cv"), control = rpart.control(cp = best.cp))

best.depth = rpartTuneDepth$results[which.max(rpartTuneDepth$results$Accuracy), 'maxdepth']

tuning.params = rpart.control(cp = best.cp, maxdepth = best.depth)

final.tree = rpart(Salary ~ ., data = Train, control = tuning.params)

prp(final.tree)
```

Immediately it is apparant how much more complex this tree is than the basic one produced by `tree()`. We also see that it includes significantly more variables than previously. We can use `varImp()` of the `caret` package to determine the relative importance of each variable based upon the reduction in the loss function attributed to each variable.

```{r, echo = F}
imp = varImp(final.tree) 
importance = data.frame(var = rownames(imp), importance = imp$Overall)

importance %>%
  arrange(desc(importance))
```

Now we will examine the differences in performance:

```{r, echo = F}
###tree
posteriors.Tree = predict(prune.5, test, type = 'vector')
predic.Tree = predict(prune.5, newdata = test, type = 'class')

df.Tree = cbind(posteriors.Tree[,2], test$Salary)
df.Tree[,2] = df.Tree[,2]-1
preds.Tree = prediction(df.Tree[,1], df.Tree[,2])
ROC.Tree = performance(preds.Tree, measure = 'tpr', x.measure = 'fpr')

auc.Tree = performance(preds.Tree, measure = 'auc')

###rpart
posteriors.rpart = predict(final.tree, test, type = 'prob')
predic.rpart = predict(final.tree, newdata = test, type = 'class')
rpart.conf = confusionMatrix(predic.rpart,test$Salary)

df.rpart = cbind(posteriors.rpart[,2], test$Salary)
preds.rpart = prediction(df.rpart[,1], df.rpart[,2])
ROC.rpart = performance(preds.rpart, measure = 'tpr', x.measure = 'fpr')

auc.rpart = performance(preds.rpart, measure = 'auc')
```


```{r, echo = F}
Metric = c('Accuracy', 'Sensitivity', 'Specificity', 'AUC')
Tree = c(tree.conf$overall[1], tree.conf$byClass[1], tree.conf$byClass[2], auc.Tree@y.values[[1]])
rPart = c(rpart.conf$overall[1], rpart.conf$byClass[1], rpart.conf$byClass[2], auc.rpart@y.values[[1]])

results = data.frame(Tree, rPart, row.names = Metric)

rpart.roc = data.frame(x = ROC.rpart@x.values[[1]], y = ROC.rpart@y.values[[1]], model = 'rPart')
tree.roc = data.frame(x = ROC.Tree@x.values[[1]], y = ROC.Tree@y.values[[1]], model = 'Tree')

rocs = rbind(rpart.roc, tree.roc)
basic = data.frame(x=c(0,1), y = c(0,1))

ggplot(data = rocs, aes(x = x, y = y)) + geom_line(aes(col = model)) + geom_line(data = basic, linetype = 2) + 
  xlab('False Positive Rate') + ylab('True Positive Rate') + ggtitle(label = 'ROC Curves', subtitle = 'Comparing tree() and rpart()')

results
```

Despite the vast differences in complexity, the trees created by `rpart()` and by `tree()` have very similar performance metrics. We can see that their ROC curves are essentially the same, with rPart performing slightly better, resulting in a ~.03 higher AUC. Similarly, rpart produces a slightly higher accuracy rating of .87 compared to .85. The most important difference though, is that rpart produces a significantly higher sensitivity while only perforing very slightly worse in specificity. Thus, these metrics lead us to conclude that the tree generated by rpart has a higher predictive power. It is important to note that the tree created by `tree()` is **much** more readable and easily interpretable than rpart's, but this reports focuses on predictive power and thus rpart produces the superior model.

##Bagged Forest
```{r, echo = F}
set.seed(34)
p = ncol(Train)-1
```

Bagging is an ensemble extension of classification trees. First, we take many random sub-samples of the data (with-replacement). For each of those sub-samples we then train a classification tree. Then, to classify a new observation, we input that observation into each tree in the forest, and each tree votes for the class that it predicts that observation to be in. Those votes are tallied over the entire forest, and the observation is assigned whichever class has the most votes.

Bagging is used to reduce variance, which is one of the weaknesses of decision trees. Trees are especially sensitive to the data they are trained on, and so training hundreds of trees on different subsets of data helps to lower the bias. 

To do this we will use the `randomForest()` function from the `randomForest` package. As we will see soon, bagged trees are a certain generalization of random forests and vice versa.

The primary parameter to tune with bagging is the number of trees to include. We will now investigate how adding trees affects the performance (AUC) of the model.

```{r}

all.trees = c(10,25,50,100,150, 200, 350, 500)
results = data.frame(ntree = all.trees, auc = 0)
for (i in 1:length(all.trees)){
  n = all.trees[i]
  forest = randomForest(Salary~.,
                        data = Train,
                        importance = F,
                        ntree = n)
  
  posteriors = predict(forest, newdata = test, type = 'prob')
  predic = predict(forest, newdata = test, type = 'class')
  df = cbind(posteriors[,2], test$Salary)
  preds = prediction(df[,1], df[,2])
  ROC = performance(preds, measure = 'tpr', x.measure = 'fpr')
  auc <-performance(preds, measure = 'auc')
  results[i,'auc'] = auc@y.values[[1]]
}
plot(results)
```

It is clear that there is diminishing returns for adding new trees to our model. Adding more trees helps us to lower the variance of our model, but it there is a point in which the gains we get from decreased variance are outweighed by complexity and computing time. From this plot we can see that a 100 tree forest will give us essentially the same results as a 500 tree forest. So, for simplicity and to reduce the complexity of our model we will use 100 trees for the following analysis.

Typically, forests such as these are grown as deeply as possible. Since we are aggregating many trees, we are less concerned with any individual tree overfitting the data, and thus there is generally no pruning of such forest methods. Now, in order to confirm this decision we will tune our model by cross validation at several different `maxnode` values. This will limit the size of each tree in our forest, and we will plot our results.

```{r, echo = F}
depths = c(2, 50, 100, seq(from = 400, to = 2400, by = 200))

results = data.frame(max.nodes = depths, auc = 0)
for (i in 1:nrow(results)){
  depth = results[i, 'max.nodes']

  forest = randomForest(Salary ~ .,
                        data = Train,
                        mtry = p,
                        importance = F,
                        ntree = 100,
                        maxnodes = depth)
  
  posteriors.Bagged = predict(forest, newdata = test, type = 'prob')
  predic.Bagged = predict(forest, newdata = test, type = 'class')
  df.Bagged = cbind(posteriors.Bagged[,2], test$Salary)
  preds.Bagged = prediction(df.Bagged[,1], df.Bagged[,2])
  ROC.Bagged = performance(preds.Bagged, measure = 'tpr', x.measure = 'fpr')
  auc.Bagged <-performance(preds.Bagged, measure = 'auc')
  results[i,'auc'] = auc.Bagged@y.values[[1]]
}

ggplot(data = results, aes(x = max.nodes, y = auc)) + geom_point() + geom_line(col = 'red')

results = results %>%
  arrange(desc(auc))
head(results)
```

From this graph we can see that at first, increasing the size of our trees *greatly* improves accuracy (AUC), but that quickly begins to level off. Still though, we can see that allowing our trees to grow larger continues to improve model accuracy, which confirms the methodology behind not pruning forests. However, in this case we achieved optimal AUC at a maxnode size of `r results[1,'max.nodes']` so we will use this max depth for our final bagged tree.

```{r, echo = F}
best.depth = results[1,'max.nodes']

final.bag = randomForest(Salary ~ .,
                         data = Train,
                         mtry = p,
                         importance = T,
                         ntree = 100,
                         maxnodes = best.depth)

posteriors.Bagged = predict(final.bag, newdata = test, type = 'prob')
predic.Bagged = predict(final.bag, newdata = test, type = 'class')
Conf.Bagged = confusionMatrix(test$Salary, predic.Bagged)
```

###Confusion Matrix and Accuracy
```{r, echo = F}
Conf.Bagged$table
c(Conf.Bagged$overall[1], Conf.Bagged$byClass[c(1,2)])
```


###ROC
```{r, echo = F}
df.Bagged = cbind(posteriors.Bagged[,2], test$Salary)
preds.Bagged = prediction(df.Bagged[,1], df.Bagged[,2])
ROC.Bagged = performance(preds.Bagged, measure = 'tpr', x.measure = 'fpr')
plot(ROC.Bagged)
abline(a=0, b=1, lty=2)

auc.Bagged <-performance(preds.Bagged, measure = 'auc')
c(AUC = auc.Bagged@y.values[[1]])
```

Immediately we can see that our bagged approach has performed better than a single classification tree. The new model has higher accuracy, higher sensititivty, much higher AUC, and a slightly lower specificity. While specific model goals may change the decisionmaking process, a bagged approach seems to have created a more powerful predictive model than a simple classification tree.

###Variable Importance
```{r, echo = F}
importance(final.bag)
varImpPlot(final.bag)
```

The most important variables are `CapitalGain`, `Relationship`, `Education`, `Age`, and `Occupation`. These are the same top five variables as our rpart tree, except that rpart used `MaritalStutus` instead of `Age`. One thing to notice here is that `FNLWGT` has a high Mean Decrease in Gini, but almost no Mean Decrease in Accuracy. This is likely because `FNLWGT` represents an imputed number representing how well reprsented that individual is in terms of how similar their census data is to those of other individuals. So, `FNLWGT` can be thought of as a numerical indicator of how common/represented an individual is. Thus, splitting on `FNLWGT` would help us to separate individuals into more pure nodes (splitting up individuals in terms of their 'normalcy'), however it does not help us with prediction, since a person's representation/uniqueness in demographic data does not necessarily correlate with their income. Thus, `FNLWGT` helps us to separate our individuals, but not to predict their income. In the future, `FNLWGT` can probably be excluded from model building since it is not a true census data point but rather an imputed measure of similarity, and it does not help our predictive models.


##Random Forest

Random forests are an extension of bagged bagging. The two follow almost exactly the same process, and have very similar goals (reducing variance). However, whereas bagging uses *all* predictors when creating each tree, a random forest takes a random subset of the predictors to be used in each tree. The number (or fraction) of predictors to use is a parameter unique to random forest: `mtry`. Again, while random forests are generally not pruned, we will confirm that choice here by tuning for both tree depth (through max number of nodes) and for mtry (number of predictors used to build each tree)

```{r, echo = F}
depths = c(50, 100, seq(from = 400, to = 2400, by = 400))
mtry = c(4,6,8,10,12)

results = expand.grid(max.nodes = depths, nvar = mtry)
results$auc = 0

for (i in 1:nrow(results)){
  depth = results[i, 'max.nodes']
  nvars = results[i, 'nvar']

  forest = randomForest(Salary ~ .,
                        data = Train,
                        mtry = nvars,
                        importance = F,
                        ntree = 100,
                        maxnodes = depth)
  
  posteriors.Forest = predict(forest, newdata = test, type = 'prob')
  predic.Forest = predict(forest, newdata = test, type = 'class')
  df.Forest = cbind(posteriors.Forest[,2], test$Salary)
  preds.Forest = prediction(df.Forest[,1], df.Forest[,2])
  ROC.Forest = performance(preds.Forest, measure = 'tpr', x.measure = 'fpr')
  auc.Forest <-performance(preds.Forest, measure = 'auc')
  results[i,'auc'] = auc.Forest@y.values[[1]]
}

results = results %>%
  arrange(desc(auc))
head(results)
```

We see that we get our best results were obtained when using the highest maxnode size we tested for. So here we will follow the general convention in our final model and allow our forest's trees to grow without pruning. Furthermore, we see that our best results were obtained when each tree was built using only 4 predictors.

Now that we have obtained our optimal parameters, we can train our final Random Forest

```{r, echo = F}
best.mtry = results[1,'nvar']

final.forest = randomForest(Salary ~ .,
                        data = Train,
                        mtry = best.mtry,
                        importance = T,
                        ntree = 100)

posteriors.Forest = predict(final.forest, newdata = test, type = 'prob')
predic.Forest = predict(final.forest, newdata = test, type = 'class')
Conf.Forest = confusionMatrix(test$Salary, predic.Forest)

Conf.Forest$table
c(Conf.Forest$overall[1], Conf.Forest$byClass[c(1,2)])
```



```{r, echo = F}
importance(final.forest)
varImpPlot(final.forest)
```

We see many of the same important variables as before: `CapitalGain`, `CapitalLoss`, `Education`, `Relationship`, `Education`, `Occupation`, and `Age`


#Model Selection

```{r, echo = F}
##Classification Tree
posteriors.Validate = predict(final.tree, Validation, type = 'prob')
predic.Validate = predict(final.tree, newdata = Validation, type = 'class')
Conf.Validate.tree =  confusionMatrix(Validation$Salary, predic.Validate)
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate.tree = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
auc.Validate.tree = performance(preds.Validate, measure = 'auc')

##Bagging
posteriors.Validate = predict(final.bag, Validation, type = 'prob')
predic.Validate = predict(final.bag, newdata = Validation, type = 'class')
Conf.Validate.bag =  confusionMatrix(Validation$Salary, predic.Validate)
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate.bag = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
auc.Validate.bag = performance(preds.Validate, measure = 'auc')

##Forest
posteriors.Validate = predict(final.forest, Validation, type = 'prob')
predic.Validate = predict(final.forest, newdata = Validation, type = 'class')
Conf.Validate.forest =  confusionMatrix(Validation$Salary, predic.Validate)
df.Validate = cbind(posteriors.Validate[,2], Validation$Salary)
preds.Validate = prediction(df.Validate[,1], df.Validate[,2])
ROC.Validate.forest = performance(preds.Validate, measure = 'tpr', x.measure = 'fpr')
auc.Validate.forest = performance(preds.Validate, measure = 'auc')
```

```{r, echo = F}

Tree = c(Conf.Validate.tree$overall[1], Conf.Validate.tree$byClass[1], Conf.Validate.tree$byClass[2], auc.Validate.tree@y.values[[1]])

Bagged = c(Conf.Validate.bag$overall[1], Conf.Validate.bag$byClass[1], Conf.Validate.bag$byClass[2], auc.Validate.bag@y.values[[1]])

Forest = c(Conf.Validate.forest$overall[1], Conf.Validate.forest$byClass[1], Conf.Validate.forest$byClass[2], auc.Validate.forest@y.values[[1]])

validate.results = data.frame(Tree, Bagged, Forest, row.names = Metric)

Tree = c(rpart.conf$overall[1], rpart.conf$byClass[1], rpart.conf$byClass[2], auc.rpart@y.values[[1]])

Bagged = c(Conf.Bagged$overall[1], Conf.Bagged$byClass[1], Conf.Bagged$byClass[2], auc.Bagged@y.values[[1]])

Forest = c(Conf.Forest$overall[1], Conf.Forest$byClass[1], Conf.Forest$byClass[2], auc.Forest@y.values[[1]])

train.results = data.frame(Tree, Bagged, Forest, row.names = Metric)  
```

###Training Results:
```{r, echo = F}
train.results
```

###Validation Results:
```{r, echo = F}
validate.results
```

From these summary results, we can see that while a randomForest is not always uniformly the best model (Tree sometimes outperforms in overall accuracy or in sensitivity) but Forest is the best model in terms of AUC for both the training data and the validation data. Visualizing the ROC curves for the validation data we can see how the Random Forest edges out the competition:

```{r, echo = F}
tree.roc = data.frame(x = ROC.Validate.tree@x.values[[1]],
                      y = ROC.Validate.tree@y.values[[1]], model = 'Tree')

bag.roc = data.frame(x = ROC.Validate.bag@x.values[[1]],
                      y = ROC.Validate.bag@y.values[[1]], model = 'Bagged')

forest.roc = data.frame(x = ROC.Validate.forest@x.values[[1]],
                      y = ROC.Validate.forest@y.values[[1]], model = 'Forest')

rocs = rbind(tree.roc, bag.roc, forest.roc)
basic = data.frame(x=c(0,1), y = c(0,1))


ggplot(data = rocs, aes(x = x, y = y)) + geom_line(aes(col = model)) + geom_line(data = basic, linetype = 2) + 
  xlab('False Positive Rate') + ylab('True Positive Rate') + ggtitle(label = 'ROC Curves', subtitle = 'Comparing Models Performance on Validation Set')
```

We can see the bagged trees and random forest models performed very similarly. This is as expected, since the two use very similar methods, but the random forest model uses some predictors for each tree while the bagged approach uses all of them. Although their performance metrics are almost identical, the random forest model performs very slightly better in each category, and thus we select the random forest model as our best classification model.

Here are the performance metrics for that model:
```{r, echo = F}
Conf.Validate.forest
```
Treating the class "over $50k a year" as the positive class, we can calculate the True Positive Rate (Sensitivity) and True Negative Rate (Specificity) manually and see that they match up with the above confusion matrix:

```{r, echo = F}
tbl = Conf.Validate.forest$table
c(Sensitivity = (tbl[1,1]/(tbl[1,1]+tbl[2,1])),
  Specificity = (tbl[2,2]/(tbl[2,2]+tbl[1,2])))
```

##Conclusions and Further Steps

We began this report with the goal of classifying whether or not an individual earns more than $50,000 a year. After constructing several classification models and comparing their performance statistics on both the training data and a witheld validation set, we determined that a random forest model with tuned parameters was the best model for this prediction task. We were able to attain an overall accuracy percentage of over 85%, and the AUC statistic for this model was above .9. These are relatively high performance metrics, and lead us to conclude that yes, the random forest model is able to accurately predict whether or not an individual earns more than 50,000 a year. Note that this is purely an exercise in model building, the decision of whether or not a model is "good enough" depends on the objective of the model. For example, there are discrepencies between teh sensitivity and specificity of the model, in part due to the uneven distribution of people above and below 50k. A different probability threshold might be set depending on which type of error is worse for a given objective. 

It should be noted that the same 5 to 6 variables were the most important in every model that we created. Similarly, several variables (such as `Race`, `Sex`, and `NativeCountry`) were consistently ranked very low in importance, and thus did not contribute much to the performance of the model. Further work may wish to exclude these variables to reduce complexity. Similarly, we discovered that while `FNLWGT` consistently ranked high for Mean Decrease in Gini, it rarely affected accuracy. As discussed earlier, `FNLWGT` is more a measure of similarity than it is a true census predictor; further work may also wish to exlude this variable as well.

##Next Steps

We were very surprised to find that `NativeCountry` was not particularly helpful in this process. Initially, we anticipated that individuals in certain regions (such as the United States) would likely earn more on average than those in others (such as South America), and thus country would be a helpful predictor. However this was not the case. It is possible that this discrepency is due to the fact that the vast majority of our individuals resided in North America, and thus our trees did not have enough variability in the data to create useful prediction splits. Further work may wish to sample international individuals at a higher rate to explore this relationship more. 

Similarly, the data was very imbalanced in regards to `Race` and `Sex`. There were more than twice as many males in this data set as females, and over 85% of individuals were White. This skewed census data can interfere with our model building **and** can create more problems when we attempt to generalize this model to a population that has different demographics than the one it was trained on. A more balanced data set would allow our model to be more generally applicable and less biased.

After completing this report, our team was curious to see how our models would have changed were we to build them using data from a different time period. For example, we are very interested to see if the same variables (`CapitalGain`, `Education`, etc) were equally important in the past, or if the factors affecting/correlated with one's income have changed. This data set was collected in 1994; we would like to perform a similar report (or perhaps a more inference-based investigation) on data from an earlier decade and data from a later decade. Comparing those results may give insight into the changing landscape of income.

